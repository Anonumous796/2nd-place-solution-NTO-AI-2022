{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2ECZlGNNxsr"
      },
      "source": [
        "# Сегментация тетрадей\n",
        "\n",
        "В данном ноутбуке представлено обучение модели instance сегментации текста в школьных тетрадях с помощью detectron2. Мыиспользовали аугментации и модель R101-FPN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvWi-9S5Nxsz"
      },
      "source": [
        "# Установка библиотек"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe7JLbyoNxs0"
      },
      "source": [
        "Установка библиотек, под которым запускается данный бейзлайн."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkUh8u9wNxs3"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.1\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1IM9KGQNxs4"
      },
      "source": [
        "# Загружаем необходимые библиотеки для создания и обучения модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ew4SJkTNxs4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9mkoS0bNxs5"
      },
      "outputs": [],
      "source": [
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf_OIoAiNxs5"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebum_MoENxs6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRzJdtRuNxs6"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import detectron2\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog,DatasetCatalog\n",
        "from detectron2.data.datasets import register_coco_instances,load_coco_json\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.engine import HookBase\n",
        "\n",
        "\n",
        "\n",
        "import logging\n",
        "logger = logging.getLogger('detectron2')\n",
        "logger.setLevel(logging.CRITICAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuI4zO0WNxs7"
      },
      "source": [
        "Прежде чем переходить к загрузке данных посмотрим, доступны ли нам GPU-мощности. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esG3ITb2Nxs8",
        "outputId": "e55b9955-1652-4b2b-d1d4-eca8eefad53d",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: True\n"
          ]
        }
      ],
      "source": [
        "print('GPU: ' + str(torch.cuda.is_available()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5oWvgLUNxs9"
      },
      "source": [
        "# Валидационный датасет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMQiLzWFsd_l"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.yandexcloud.net/datasouls-competitions/ai-nto-final-2022/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "vSu22k2qsp2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmqwVPlnNxs9"
      },
      "source": [
        "Для валидации наших моделей нам неплохо было создать из обучающих данных валидационный датасет. Для этого разделим наш датасет на две части - для обучения и для валидации. Для этого просто создадим два новых файлика с аннотациями, куда раздельно запишем исиходную информацию об аннотациях."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zmJyPvBNxtD"
      },
      "source": [
        "# Регистрация датасета"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KThaLpoiNxtE"
      },
      "source": [
        "Зарегистрируем выборки в detectron2 для дальнейшей подачи на обучение модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvoerHuvNxtF"
      },
      "outputs": [],
      "source": [
        "for d in ['train','val']:\n",
        "    DatasetCatalog.register(\"my_dataset2_\"+d, lambda d=d: load_coco_json(\"data/train_segmentation/annotations.json\",\n",
        "    image_root= \"data/train_segmentation/images\",\\\n",
        "    dataset_name=\"my_dataset2_\"+d,extra_annotation_keys=['bbox_mode']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulCxsS_sNxtG"
      },
      "source": [
        "После регистрации можно загружать выборки, чтобы иметь возможность посмотреть на них глазами. Первой загрузим обучающую выборку в **dataset_dicts_train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h9OfC8ENxtG"
      },
      "outputs": [],
      "source": [
        "dataset_dicts_train = DatasetCatalog.get(\"my_dataset2_train\")\n",
        "train_metadata = MetadataCatalog.get(\"my_dataset2_train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BDQFyVFNxtG"
      },
      "source": [
        "И тестовую выборку в **dataset_dicts_val**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN-9QRBYNxtG"
      },
      "outputs": [],
      "source": [
        "dataset_dicts_val = DatasetCatalog.get(\"my_dataset2_val\")\n",
        "val_metadata = MetadataCatalog.get(\"my_dataset2_val\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7j66I8ONxtH"
      },
      "source": [
        "Посмотрим на размер получившихся выборок - эта операция в python осуществляется при помощи функции **len()**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulg5WTQwNxtH",
        "outputId": "26e4f2e6-2b49-454c-fcfc-f8409c74a9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер обучающей выборки (Картинки): 932\n",
            "Размер тестовой выборки (Картинки): 932\n"
          ]
        }
      ],
      "source": [
        "print('Размер обучающей выборки (Картинки): {}'.format(len(dataset_dicts_train)))\n",
        "print('Размер тестовой выборки (Картинки): {}'.format(len(dataset_dicts_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEU_fBscNxtH"
      },
      "source": [
        "Итак, у нас в распоряжении 588 изображения для тренировки, и 66 - для проверки качества."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHlx8i9wNxtH"
      },
      "source": [
        "**Посмотрим на размеченные фотографии из валидации**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7ybje_iNxtH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.display import Image\n",
        "@interact\n",
        "def show_images(file=range(len(dataset_dicts_val))):\n",
        "    example = dataset_dicts_val[file]\n",
        "    image = utils.read_image(example[\"file_name\"], format=\"RGB\")\n",
        "    plt.figure(figsize=(3,3),dpi=200)\n",
        "    visualizer = Visualizer(image[:, :, ::-1], metadata=val_metadata, scale=0.5)\n",
        "    vis = visualizer.draw_dataset_dict(example)\n",
        "    plt.imshow(vis.get_image()[:, :,::-1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyqzYZWpNxtI"
      },
      "source": [
        "## Обучение модели\n",
        "\n",
        "**4.1. Определяем конфигурацию**\n",
        "\n",
        "Прежде чем начать работать с самой моделью, нам нужно определить ее параметры и спецификацию обучения\n",
        "\n",
        "Создаем конфигурацию и загружаем архитектуру модели с предобученными весами (на COCO - датасете, содержащем $80$ популярных категорий объектов и более $300000$ изображений) для распознавания объектов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN0FxwBWNxtI"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")) \n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s0ZuDymNxtI"
      },
      "source": [
        "В целом, вы можете посмотреть и другие архитектуры в зоопарке [моделей](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q9HKEQcNxtK"
      },
      "source": [
        "Теперь задаем параметры самой модели и обучения модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82X5sXc6CJVB"
      },
      "outputs": [],
      "source": [
        "# Загружаем названия обучающией и тестовой выборок в настройки\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset2_train\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset2_val\",)\n",
        "cfg.TEST.EVAL_PERIOD = 500000\n",
        "cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
        "cfg.INPUT.MIN_SIZE_TEST= 1960\n",
        "cfg.INPUT.MAX_SIZE_TEST = 2016\n",
        "# Часто имеет смысл сделать изображения чуть меньшего размера, чтобы \n",
        "# обучение происходило быстрее. Поэтому мы можем указать размер, до которого будем изменяться наименьшая \n",
        "# и наибольшая из сторон исходного изображения.\n",
        "cfg.INPUT.MIN_SIZE_TRAIN = 1960\n",
        "cfg.INPUT.MAX_SIZE_TRAIN = 2016\n",
        "\n",
        "# Также мы должны сказать модели ниже какой вероятности определения она игнорирует результат. \n",
        "# То есть, если она найдет на картинке еду, но вероятность правильного определения ниже 0.5, \n",
        "# то она не будет нам сообщать, что она что-то нашла.\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "# Также мы должны указать порядок каналов во входном изображении. Обратите внимание, что это Blue Green Red (BGR), \n",
        "# а не привычный RGB. Это особенности работы данной модели.\n",
        "cfg.INPUT.FORMAT = 'BGR' \n",
        "\n",
        "# Для более быстрой загрузки данных в модель, мы делаем параллельную загрузку. Мы указываем параметр 4, \n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 1024\n",
        "cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = 256\n",
        "\n",
        "# Следующий параметр задает количество изображений в батче, на котором \n",
        "# модель делает одну итерацию обучения (изменения весов).\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "\n",
        "# Зададим также learning_rate\n",
        "cfg.SOLVER.BASE_LR = 0.01\n",
        "\n",
        "# Укажем модели, через сколько шагов обучения модели следует уменьшить learning rate\n",
        "cfg.SOLVER.STEPS = (24000,38000,50000)\n",
        "\n",
        "# Фактор, на который уменьшается learning rate задается следующим выражением\n",
        "cfg.SOLVER.GAMMA = 0.5\n",
        "#cfg.SOLVER.WEIGHT_DECAY = 0.01\n",
        "\n",
        "# Зададим общее число итераций обучения.\n",
        "cfg.SOLVER.MAX_ITER = 58000\n",
        "\n",
        "# Укажем количество классов в нашей выборке\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "\n",
        "# Задаем через сколько  шагов обучения сохранять веса модели в файл. Этот файл мы сможем загрузить потом \n",
        "# для тестирования нашей обученной модели на новых данных.\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 2000\n",
        "\n",
        "# И указываем название папки, куда сохранять чекпойнты модели и информацию о процессе обучения.\n",
        "cfg.OUTPUT_DIR = 'outputs'\n",
        "\n",
        "# Если вдруг такой папки нет, то создадим ее\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Если мы хотим удалить чекпойнты предыдущих моделей, то выполняем данную команду. \n",
        "#%rm output/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjvL82qzNxtK"
      },
      "source": [
        "**4.2. Обучаем модель**\n",
        "\n",
        "Процесс обучения модели запускают следующие три строчки кода. Возможно будут предупреждения, на которые можно не обращать внимания, это информация об обучении."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "WsaM08Jj0BjE",
        "outputId": "5e81d8a9-76a3-421a-c089-dfe4648ed7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.0.0\n",
            "  Downloading albumentations-1.0.0-py3-none-any.whl (98 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 2.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (1.21.5)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (0.18.3)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 112 kB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (5.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (1.2.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.2.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.15.0)\n",
            "Installing collected packages: opencv-python-headless, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.0 opencv-python-headless-4.5.5.62\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: opencv-python-headless 4.5.5.62\n",
            "Uninstalling opencv-python-headless-4.5.5.62:\n",
            "  Successfully uninstalled opencv-python-headless-4.5.5.62\n",
            "Collecting opencv-python-headless==4.1.2.30\n",
            "  Downloading opencv_python_headless-4.1.2.30-cp37-cp37m-manylinux1_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 179 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.1.2.30) (1.21.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "Successfully installed opencv-python-headless-4.1.2.30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install albumentations==1.0.0\n",
        "!pip uninstall opencv-python-headless==4.5.5.62 -y\n",
        "!pip install opencv-python-headless==4.1.2.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NCp_jA_htEM"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "a_transforms = A.Compose([\n",
        "                        A.OneOf([\n",
        "                             A.RGBShift(p=1),\n",
        "                             A.HueSaturationValue(p=1),\n",
        "                             A.CLAHE(p=1),\n",
        "                        ], p=0.2),\n",
        "                        A.OneOf([\n",
        "                             A.Blur(blur_limit=7,p=1),\n",
        "                             A.GaussianBlur(p=1),\n",
        "                             A.MedianBlur (blur_limit=7,p=1),\n",
        "                        ], p=0.2),\n",
        "                        A.OneOf([\n",
        "                             A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=1),\n",
        "                             A.RandomGamma (gamma_limit=(80, 150), p=1),\n",
        "                             A.RandomToneCurve(p=1),\n",
        "                        ], p=0.2),\n",
        "                        A.OneOf([\n",
        "                             A.ColorJitter(p=1),\n",
        "                             A.JpegCompression(p=1),\n",
        "                             A.GaussNoise(p=1),\n",
        "                        ], p=0.2),\n",
        "                        A.RandomShadow (p=0.2),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7s0uTUhg2Vm"
      },
      "outputs": [],
      "source": [
        "import detectron2.data.transforms as T\n",
        "import copy\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n",
        "def custom_mapper(dataset_dict):\n",
        "    # Implement a mapper, similar to the default DatasetMapper, but with your own customizations\n",
        "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
        "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
        "    transform_list = [T.ResizeShortestEdge([1960, 1960], 2016),\n",
        "                      T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n",
        "                      T.RandomFlip(prob=0.5, horizontal=True, vertical=False), \n",
        "                      T.RandomRotation(angle=[-45, 45]),\n",
        "                      T.RandomCrop(crop_type='relative_range', crop_size=[0.8, 0.8])\n",
        "                      ]\n",
        "    image, transforms = T.apply_transform_gens(transform_list, image)\n",
        "    image = a_transforms(image=image)['image']\n",
        "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
        "\n",
        "    annos = [\n",
        "        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
        "        for obj in dataset_dict.pop(\"annotations\")\n",
        "        if obj.get(\"iscrowd\", 0) == 0\n",
        "    ]\n",
        "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
        "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
        "    return dataset_dict\n",
        "\n",
        "\n",
        "class CustomTrainer(DefaultTrainer):\n",
        "    \n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        return build_detection_train_loader(cfg, mapper=custom_mapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BETTDOZaNxtK"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHpKzC8wNxtL"
      },
      "source": [
        "Используем обученную модель для проверки качества на валидации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnBgruc8NxtL"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = \"outputs/model_final.pth\"\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
        "cfg.DATASETS.TEST = (\"my_dataset_val\", )\n",
        "#Изменение размера исходных изображений для тестового датасета\n",
        "cfg.INPUT.MIN_SIZE_TEST= 1960\n",
        "cfg.INPUT.MAX_SIZE_TEST = 2016\n",
        "cfg.INPUT.FORMAT = 'BGR'\n",
        "\n",
        "#ВАЖНО увеличить это значение (стандартное равно 100). Так как на листе тетради может быть довольно много слов\n",
        "cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
        "\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7drtKIeNxtL"
      },
      "source": [
        "Сделаем предсказания для тестового датасета и сразу же нарисуем его.\n",
        "\n",
        "Вы можете выбрать из выпадающего списка номер изображения, и посмотреть разметку на всем валидационном датасете."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGe_OTmCNxtM",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "@interact\n",
        "def show_images(file=range(len(dataset_dicts_val))):\n",
        "    \n",
        "    example = dataset_dicts_val[file]\n",
        "    im = cv2.imread(example[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    fig, axs = plt.subplots(nrows=1,ncols=2,figsize=(4,4),dpi=200)\n",
        "    v = Visualizer(im[:, :],\n",
        "                  metadata=val_metadata, \n",
        "                  scale=0.4 )\n",
        "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    axs[0].imshow(im[:, :, ::-1])\n",
        "    axs[1].imshow(v.get_image()[:, :, ::-1])\n",
        "    axs[0].axis('off')\n",
        "    axs[1].axis('off')\n",
        "    axs[0].set_title('Original')\n",
        "    axs[1].set_title('Predict')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdcFDUMnNxtM"
      },
      "source": [
        "Можно непосредственно в коде изменить номер изображения, которое Вы хотите обработать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IG-2iFLNxtN"
      },
      "outputs": [],
      "source": [
        "id_image_selected = 3\n",
        "example = dataset_dicts_val[id_image_selected]\n",
        "im = cv2.imread(example[\"file_name\"])\n",
        "outputs = predictor(im)\n",
        "plt.figure(figsize=(7,7))\n",
        "v = Visualizer(im[:, :],\n",
        "              metadata=val_metadata, \n",
        "              scale=0.4 )\n",
        "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "plt.imshow(v.get_image()[:, :, ::-1])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cgNpqtBXLUV"
      },
      "outputs": [],
      "source": [
        "outputs['instances']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goS8Uu1FNxtN"
      },
      "source": [
        "В качестве предсказаний для каждого изображения из тестового набора требуется получить бинарную маску, в которой `1` означает, что данный пиксель относится к классу текста."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aLo7PJuNxtN"
      },
      "source": [
        "Давайте на примере одного изображения переведем формат выхода Detectron2 в требуемый формат для соревнования.\n",
        "\n",
        "`outputs` - результат предсказания модели на данном изображении из предыдущего блока с кодом"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaLVop7MNxtO"
      },
      "outputs": [],
      "source": [
        "prediction = outputs['instances'].pred_masks.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8XmyOM0NxtO"
      },
      "outputs": [],
      "source": [
        "prediction.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMcOZhUoNxtO"
      },
      "source": [
        "В `prediction` находится массив бинарных матриц. Каждая матрица отвечает за отдельную задетектированную маску текста. В нашем случае модель задетектировала 80 текстовых масок. Давайте провизуализируем одну из них."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml1JE6yqNxtP"
      },
      "outputs": [],
      "source": [
        "prediction[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5PJ1x9PNxtP"
      },
      "outputs": [],
      "source": [
        "plt.imshow(prediction[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ecFzHhNxtQ"
      },
      "source": [
        "Отлично. Теперь, для того, чтобы получить бинарную маску со всем задетектированным текстом для изображения, нам нужно объединить все маски в одну. Для этого мы просто поэелементно сложим все наши матрицы. Там, где после сложения остались нули - модель не задетектировала никакого текста."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzEjSfaXNxtQ"
      },
      "outputs": [],
      "source": [
        "mask = np.add.reduce(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5gNCrO8NxtQ"
      },
      "outputs": [],
      "source": [
        "mask = mask > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDKTib2DNxtR"
      },
      "outputs": [],
      "source": [
        "plt.imshow(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4n9KuTPNxtR"
      },
      "source": [
        "Итак, нам нужно полуить такую маску для каждого изображения из валидационной выборки, а затем посчитать метрику F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOHbDozjNxtS"
      },
      "outputs": [],
      "source": [
        "#Подгрузим аннотации train\n",
        "with open('data/val/annotations_new.json') as f:\n",
        "    annotations_val = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa4l7hc1NxtS"
      },
      "outputs": [],
      "source": [
        "val_images = annotations_val['images']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxDHiMu3NxtS"
      },
      "outputs": [],
      "source": [
        "val_predictions = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kQTaC72YNxtT"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  for val_img in tqdm.tqdm_notebook(val_images):\n",
        "      file_name = val_img['file_name']\n",
        "      img_path = os.path.join('data/val/images/',file_name)\n",
        "      im = cv2.imread(img_path)\n",
        "      outputs = predictor(im)\n",
        "      prediction = outputs['instances'].pred_masks.cpu().numpy()\n",
        "      mask = np.add.reduce(prediction)\n",
        "      mask = mask > 0\n",
        "      val_predictions[file_name] = mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErUhD6ENNxtU"
      },
      "source": [
        "Для сохрания предсказаний и загрузки бинарных масок бы будет использовать формат `.npz`. Он позволяет хранить большие массивы в компактном виде. Вот [ссылка](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html) на документацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JytYgBCNxtU"
      },
      "outputs": [],
      "source": [
        "np.savez_compressed('val_pred.npz',**val_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAOncf-NxtV"
      },
      "source": [
        "Подгрузим бинарные маски для train и val (только что сохраненную). Так как мы в начале бейзлайна разбивали весь исходный train на новый трейн и валидацию, то информация по всем маскам из исходного train хранится в `binary.npz`. \n",
        "\n",
        "Получившийся после подгрузки `np.load()` - что то вроде словаря. Его ключи можно получить с помощью метода files - `loaded_val.files`. В нашем случае ключами являются ключи исходного словаря `val_predictions`, то есть названия изображений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1wKcYiJNxtV"
      },
      "outputs": [],
      "source": [
        "loaded_train = np.load('train_data/binary.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Xoch-BNxtV"
      },
      "outputs": [],
      "source": [
        "loaded_val_pred = np.load('val_pred.npz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_QNM7RgNxtV"
      },
      "source": [
        "Мы используем среднюю метрика F1-score. То есть считаем F1-score для каждого изображения, а затем усредняем результаты. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHw6Ot6ONxtV"
      },
      "source": [
        "Реализация из sklearn работает довольно долго, попэтому мы будем использовать свою."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sXGeJd8NxtX"
      },
      "outputs": [],
      "source": [
        "def f1_loss(y_true, y_pred):\n",
        "        \n",
        "    \n",
        "    tp = np.sum(y_true & y_pred)\n",
        "    tn = np.sum(~y_true & ~y_pred)\n",
        "    fp = np.sum(~y_true & y_pred)\n",
        "    fn = np.sum(y_true & ~y_pred)\n",
        "    \n",
        "    epsilon = 1e-7\n",
        "    \n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "    \n",
        "    f1 = 2* precision*recall / ( precision + recall + epsilon)\n",
        "\n",
        "    return f1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv80tlyBNxtX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "f1_scores = []\n",
        "for key in tqdm.tqdm_notebook(loaded_val_pred.files):\n",
        "    pred = loaded_val_pred[key].reshape(-1)\n",
        "    true = loaded_train[key].reshape(-1)\n",
        "    \n",
        "    f1_img = f1_loss(true,pred)\n",
        "    f1_scores.append(f1_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BU8OgjsNxtY"
      },
      "source": [
        "Получившаяся метрика на валидации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNPpNLllNxtY"
      },
      "outputs": [],
      "source": [
        "np.mean(f1_scores)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "model_for_detection",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}